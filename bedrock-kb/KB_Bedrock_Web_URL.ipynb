{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Bases for Amazon Bedrock with Web URL Data Connector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U opensearch-py==2.3.1\n",
    "# %pip install -U boto3==1.34.143\n",
    "# %pip install -U retrying==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "Firt of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss\n",
    "import random\n",
    "from retrying import retry\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "service = 'aoss'\n",
    "\n",
    "bucket_name = <PUT YOUR BUCKET NAME> # Provide your bucket name which is already created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "vector_store_name = f'bedrock-sample-rag-{suffix}'\n",
    "index_name = f\"bedrock-sample-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss( vector_store_name=vector_store_name,\n",
    "                                                                           aoss_client=aoss_client,\n",
    "                                                                           bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createCollectionDetail': {'arn': 'arn:aws:aoss:us-east-1:507922848584:collection/n7bvyeb0mdj42h0spssj', 'createdDate': 1721171086378, 'id': 'n7bvyeb0mdj42h0spssj', 'kmsKeyArn': 'auto', 'lastModifiedDate': 1721171086378, 'name': 'bedrock-sample-rag-683', 'standbyReplicas': 'ENABLED', 'status': 'CREATING', 'type': 'VECTORSEARCH'}, 'ResponseMetadata': {'RequestId': '79d69662-6df5-4b6c-9232-87d0db99b0c4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '79d69662-6df5-4b6c-9232-87d0db99b0c4', 'date': 'Tue, 16 Jul 2024 23:04:46 GMT', 'content-type': 'application/x-amz-json-1.0', 'content-length': '314', 'connection': 'keep-alive'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(collection)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n7bvyeb0mdj42h0spssj.us-east-1.aoss.amazonaws.com\n"
     ]
    }
   ],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::507922848584:policy/AmazonBedrockOSSPolicyForKnowledgeBase_797\n"
     ]
    }
   ],
   "source": [
    "# create oss policy and attach it to Bedrock execution role\n",
    "create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                bedrock_kb_execution_role=bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "            \"method\": {\n",
    "                \"name\": \"hnsw\",\n",
    "                \"engine\": \"faiss\",  \n",
    "                \"space_type\": \"l2\",\n",
    "                \"parameters\": {\n",
    "                    \"ef_construction\": 200,\n",
    "                    \"m\": 16\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'bedrock-sample-index-683'}\n"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "print('\\nCreating index:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the web URL configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "    \"fixedSizeChunkingConfiguration\": {\n",
    "        \"maxTokens\": 512,\n",
    "        \"overlapPercentage\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "webConfiguration = {\"sourceConfiguration\": {\n",
    "                          \"urlConfiguration\": {\n",
    "                           \"seedUrls\": [{\n",
    "                                    \"url\": \"https://www.datascienceportfol.io/suman\"                  #### <<<<<------ <Change this to your Web URL> \n",
    "                                }]\n",
    "                            }\n",
    "                        },\n",
    "                     \"crawlerConfiguration\": {\n",
    "                            \"crawlerLimits\": {\n",
    "                                \"rateLimit\": 50\n",
    "                            },\n",
    "                            \"scope\": \"HOST_ONLY\"\n",
    "                        }\n",
    "                   }         \n",
    "\n",
    "\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Bedrock Knowledge Bases for Web URL Connector\"\n",
    "roleArn = bedrock_kb_execution_role_arn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createdAt': datetime.datetime(2024, 7, 16, 23, 7, 54, 679021, tzinfo=tzutc()), 'description': 'Bedrock Knowledge Bases for Web URL Connector', 'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:507922848584:knowledge-base/1YIB0D2DOM', 'knowledgeBaseConfiguration': {'type': 'VECTOR', 'vectorKnowledgeBaseConfiguration': {'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'}}, 'knowledgeBaseId': '1YIB0D2DOM', 'name': 'bedrock-sample-knowledge-base-683', 'roleArn': 'arn:aws:iam::507922848584:role/AmazonBedrockExecutionRoleForKnowledgeBase_797', 'status': 'CREATING', 'storageConfiguration': {'opensearchServerlessConfiguration': {'collectionArn': 'arn:aws:aoss:us-east-1:507922848584:collection/n7bvyeb0mdj42h0spssj', 'fieldMapping': {'metadataField': 'text-metadata', 'textField': 'text', 'vectorField': 'vector'}, 'vectorIndexName': 'bedrock-sample-index-683'}, 'type': 'OPENSEARCH_SERVERLESS'}, 'updatedAt': datetime.datetime(2024, 7, 16, 23, 7, 54, 679021, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "print(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Just for documentation :) \n",
    "# help(bedrock_agent_client.create_data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createdAt': datetime.datetime(2024, 7, 16, 23, 8, 5, 454853, tzinfo=tzutc()), 'dataDeletionPolicy': 'DELETE', 'dataSourceConfiguration': {'type': 'WEB', 'webConfiguration': {'crawlerConfiguration': {'crawlerLimits': {'rateLimit': 50}, 'scope': 'HOST_ONLY'}, 'sourceConfiguration': {'urlConfiguration': {'seedUrls': [{'url': 'https://www.datascienceportfol.io/suman'}]}}}}, 'dataSourceId': 'ZQHAKAWVXV', 'description': 'Bedrock Knowledge Bases for Web URL Connector', 'knowledgeBaseId': '1YIB0D2DOM', 'name': 'bedrock-sample-knowledge-base-683', 'status': 'AVAILABLE', 'updatedAt': datetime.datetime(2024, 7, 16, 23, 8, 5, 454853, tzinfo=tzutc()), 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE', 'fixedSizeChunkingConfiguration': {'maxTokens': 512, 'overlapPercentage': 20}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataDeletionPolicy = 'DELETE',\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"WEB\",\n",
    "        \"webConfiguration\":webConfiguration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'c572864d-d8d9-443a-8c5d-33456095e7a5',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Tue, 16 Jul 2024 23:08:29 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '734',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'c572864d-d8d9-443a-8c5d-33456095e7a5',\n",
       "   'x-amz-apigw-id': 'bByJFEBroAMEkjA=',\n",
       "   'x-amzn-trace-id': 'Root=1-6696fd6d-651c5ab273c33e26510d2e8e'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'createdAt': datetime.datetime(2024, 7, 16, 23, 8, 5, 454853, tzinfo=tzutc()),\n",
       "  'dataDeletionPolicy': 'DELETE',\n",
       "  'dataSourceConfiguration': {'type': 'WEB',\n",
       "   'webConfiguration': {'crawlerConfiguration': {'crawlerLimits': {'rateLimit': 50},\n",
       "     'scope': 'HOST_ONLY'},\n",
       "    'sourceConfiguration': {'urlConfiguration': {'seedUrls': [{'url': 'https://www.datascienceportfol.io/suman'}]}}}},\n",
       "  'dataSourceId': 'ZQHAKAWVXV',\n",
       "  'description': 'Bedrock Knowledge Bases for Web URL Connector',\n",
       "  'knowledgeBaseId': '1YIB0D2DOM',\n",
       "  'name': 'bedrock-sample-knowledge-base-683',\n",
       "  'status': 'AVAILABLE',\n",
       "  'updatedAt': datetime.datetime(2024, 7, 16, 23, 8, 5, 454853, tzinfo=tzutc()),\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'FIXED_SIZE',\n",
       "    'fixedSizeChunkingConfiguration': {'maxTokens': 512,\n",
       "     'overlapPercentage': 20}}}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataSourceId': 'ZQHAKAWVXV', 'ingestionJobId': 'LWC9QQUIVA', 'knowledgeBaseId': '1YIB0D2DOM', 'startedAt': datetime.datetime(2024, 7, 16, 23, 8, 33, 484474, tzinfo=tzutc()), 'statistics': {'numberOfDocumentsDeleted': 0, 'numberOfDocumentsFailed': 0, 'numberOfDocumentsScanned': 0, 'numberOfMetadataDocumentsModified': 0, 'numberOfMetadataDocumentsScanned': 0, 'numberOfModifiedDocumentsIndexed': 0, 'numberOfNewDocumentsIndexed': 0}, 'status': 'STARTING', 'updatedAt': datetime.datetime(2024, 7, 16, 23, 8, 33, 484474, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataSourceId': 'ZQHAKAWVXV', 'failureReasons': ['[\"Skipped document: https://github.com/debnsuma/masters-ml/blob/main/Case_study/01_SQL_Target/Business%20Case_%20Target%20SQL.pdf. The resource you are requesting doesn\\'t exist.\"]'], 'ingestionJobId': 'LWC9QQUIVA', 'knowledgeBaseId': '1YIB0D2DOM', 'startedAt': datetime.datetime(2024, 7, 16, 23, 8, 33, 484474, tzinfo=tzutc()), 'statistics': {'numberOfDocumentsDeleted': 0, 'numberOfDocumentsFailed': 0, 'numberOfDocumentsScanned': 12, 'numberOfMetadataDocumentsModified': 0, 'numberOfMetadataDocumentsScanned': 0, 'numberOfModifiedDocumentsIndexed': 0, 'numberOfNewDocumentsIndexed': 11}, 'status': 'COMPLETE', 'updatedAt': datetime.datetime(2024, 7, 16, 23, 14, 12, 871316, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "  get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "  job = get_job_response[\"ingestionJob\"]\n",
    "print(job)\n",
    "time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1YIB0D2DOM\n"
     ]
    }
   ],
   "source": [
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "print(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test the knowledge base\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"                                           # <Change it to any model of your choice which is supported by KB>\n",
    "model_arn = f'arn:aws:bedrock:us-east-1::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, some of the projects Suman Debnath has worked on include:\n",
      "\n",
      "- Comprehensive e-commerce analytics for a leading retailer in Brazil, involving data analysis, visualization, and identifying trends and seasonal variances in order data.\n",
      "- Strategic media content analysis for a leading video streaming platform, using data analysis and visualization to drive content development and market expansion decisions.\n",
      "- Customer segmentation analysis for a major fitness equipment brand, using statistical analysis to define target customer profiles for treadmill products.\n",
      "- Statistical customer behavior analysis for a major retail corporation, analyzing Black Friday transaction data to compare spending behaviors across demographics. Additionally, Suman has worked on:\n",
      "\n",
      "- Efficient feature engineering for a leading logistics organization, cleaning and engineering features from raw data to enhance forecasting models and improve operational efficiency.\n",
      "- Creating a Tableau dashboard providing insights into AWS service engagement on Stack Overflow, using distributed systems like Apache Spark and data processing tools like Amazon EMR, Glue and SageMaker.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the projects Suman has worked on?\"\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "generated_text = response['output']['text']\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Suman Debnath | Principal Developer Advocate (AI/ML) | Data portfolio Suman Debnath Principal Developer Advocate (AI/ML) Based in Boston, US LinkedIn](https://www.linkedin.com/in/suman-d/)[ email me About Suman Debnath is a Principal Machine Learning Advocate at Amazon Web Services. He transitioned to machine learning in 2020 after a career in systems, storage and performance engineering. Currently, his focus is on Supervised Learning, Natural Language Processing (NLP), Large Language Models (LLMs), and Retrieval Augmented Generation (RAG). Suman is committed to leveraging open-source tools like TensorFlow, PyTorch, Numpy, Spark, and Pandas for advancing machine learning. He has developed performance benchmarking and monitoring tools for distributed storage systems. Suman has spoken at over 250 global events, including PyCon, PyData, ODSC, and meetups across multiple countries. At present, he leads the Global Developer Advocate team for Machine Learning and GenAI at AWS. Skills Python SQL AWS PyTorch TensorFlow Amazon Bedrock Amazon SageMaker Amazon EFS Linear Algebra Probability Deep Learning NLP LLM LangChain Docker Apache Spark Numpy Pandas Supervised Machine Learning Embedding Vector Database Amazon Aurora RAG GenAI Projects Project photo Comprehensive E-commerce Analytics for a Leading Retailer in Brazil Data Analysis Data Visualization SQL Python Statistical Analysis Critical Thinking Problem Solving Performed comprehensive data analysis on 100,000 orders for a leading retailer in Brazil, focusing on order statuses, pricing, and customer demographics. Utilized SQL and Python to conduct exploratory and advanced analytics, revealing key trends and seasonal variances. Read more Project photo Strategic Media Content Analysis for a Leading Video Streaming Platform Data Analysis Data Visualization Python Pandas Matplotlib Seaborn Statistical Analysis Strategic Planning Problem Solving Analyzed over 10,000 media titles on a leading video streaming platform, focusing on content distribution and viewer preferences. Utilized data cleaning, exploratory analysis, and visualization to drive strategic content development and market expansion decisions. Read more Project photo Customer Segmentation Analysis for a Leading Fitness Equipment Brand Data Analysis Statistical Analysis Python Pandas Matplotlib Seaborn Descriptive Statistics Probability Calculation Data Visualization In this project, I conducted a thorough statistical analysis for a major fitness equipment manufacturer to define the target customer profiles for their treadmill products. Analyzed customer purchase data to construct detailed customer segments using descriptive statistics, contingency tables/probs. Read more Project photo Statistical Customer Behavior Analysis for a Major Retail Corporation Statistical Analysis Data Analysis Confidence Intervals Central Limit Theorem Python Pandas Data Visualization Analyzed Black Friday transaction data to compare spending behaviors across different demographics at a leading retail corporation.', \"Suman Debnath | Efficient Feature Engineering for a Leading Logistics Organization Suman Debnath Principal Developer Advocate (AI/ML) Based in Boston, US LinkedIn](https://www.linkedin.com/in/suman-d/)[ email me About Suman Debnath is a Principal Machine Learning Advocate at Amazon Web Services. He transitioned to machine learning in 2020 after a career in systems, storage and performance engineering. Currently, his focus is on Supervised Learning, Natural Language Processing (NLP), Large Language Models (LLMs), and Retrieval Augmented Generation (RAG). Suman is committed to leveraging open-source tools like TensorFlow, PyTorch, Numpy, Spark, and Pandas for advancing machine learning. He has developed performance benchmarking and monitoring tools for distributed storage systems. Suman has spoken at over 250 global events, including PyCon, PyData, ODSC, and meetups across multiple countries. At present, he leads the Global Developer Advocate team for Machine Learning and GenAI at AWS. Skills Python SQL AWS PyTorch TensorFlow Amazon Bedrock Amazon SageMaker Amazon EFS Linear Algebra Probability Deep Learning NLP LLM LangChain Docker Apache Spark Numpy Pandas Supervised Machine Learning Embedding Vector Database Amazon Aurora RAG GenAI back Efficient Feature Engineering for a Leading Logistics Organization Project photo Project links GitHub Skills Feature Engineering Data Cleaning Exploratory Data Analysis Python Pandas Data Manipulation About this project Cleaned and engineered features from raw data to enhance forecasting models for a leading logistics company's operations. Conducted in-depth analysis, including outlier detection, normalization, and categorical value handling, to optimize delivery route planning and improve operational efficiency.\", 'Suman Debnath | StackOverflow AWS Engagement Insights Dashboard Suman Debnath Principal Developer Advocate (AI/ML) Based in Boston, US LinkedIn](https://www.linkedin.com/in/suman-d/)[ email me About Suman Debnath is a Principal Machine Learning Advocate at Amazon Web Services. He transitioned to machine learning in 2020 after a career in systems, storage and performance engineering. Currently, his focus is on Supervised Learning, Natural Language Processing (NLP), Large Language Models (LLMs), and Retrieval Augmented Generation (RAG). Suman is committed to leveraging open-source tools like TensorFlow, PyTorch, Numpy, Spark, and Pandas for advancing machine learning. He has developed performance benchmarking and monitoring tools for distributed storage systems. Suman has spoken at over 250 global events, including PyCon, PyData, ODSC, and meetups across multiple countries. At present, he leads the Global Developer Advocate team for Machine Learning and GenAI at AWS. Skills Python SQL AWS PyTorch TensorFlow Amazon Bedrock Amazon SageMaker Amazon EFS Linear Algebra Probability Deep Learning NLP LLM LangChain Docker Apache Spark Numpy Pandas Supervised Machine Learning Embedding Vector Database Amazon Aurora RAG GenAI back StackOverflow AWS Engagement Insights Dashboard Project photo Project links Tableau ](https://public.tableau.com/app/profile/suman.debnath7118/viz/StackOverflowAWSAnalysisDashboard/Story1) [GitHub Skills Tableau Apache Spark PySpark EMR Glue SageMaker About this project Created a Tableau dashboard providing insights into AWS service-wise engagement on Stack Overflow. Leveraged distributed systems (Apache Spark) and PySpark for data processing (Amazon EMR, Glue and SageMaker) and cleaning, and utilized Numpy and Pandas for data analysis.']\n"
     ]
    }
   ],
   "source": [
    "## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "        contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# retreive api for fetching only the relevant context.\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': {'text': 'Suman Debnath | StackOverflow AWS Engagement Insights Dashboard Suman Debnath Principal Developer Advocate (AI/ML) Based in Boston, US LinkedIn](https://www.linkedin.com/in/suman-d/)[ email me About Suman Debnath is a Principal Machine Learning Advocate at Amazon Web Services. He transitioned to machine learning in 2020 after a career in systems, storage and performance engineering. Currently, his focus is on Supervised Learning, Natural Language Processing (NLP), Large Language Models (LLMs), and Retrieval Augmented Generation (RAG). Suman is committed to leveraging open-source tools like TensorFlow, PyTorch, Numpy, Spark, and Pandas for advancing machine learning. He has developed performance benchmarking and monitoring tools for distributed storage systems. Suman has spoken at over 250 global events, including PyCon, PyData, ODSC, and meetups across multiple countries. At present, he leads the Global Developer Advocate team for Machine Learning and GenAI at AWS. Skills Python SQL AWS PyTorch TensorFlow Amazon Bedrock Amazon SageMaker Amazon EFS Linear Algebra Probability Deep Learning NLP LLM LangChain Docker Apache Spark Numpy Pandas Supervised Machine Learning Embedding Vector Database Amazon Aurora RAG GenAI back StackOverflow AWS Engagement Insights Dashboard Project photo Project links Tableau ](https://public.tableau.com/app/profile/suman.debnath7118/viz/StackOverflowAWSAnalysisDashboard/Story1) [GitHub Skills Tableau Apache Spark PySpark EMR Glue SageMaker About this project Created a Tableau dashboard providing insights into AWS service-wise engagement on Stack Overflow. Leveraged distributed systems (Apache Spark) and PySpark for data processing (Amazon EMR, Glue and SageMaker) and cleaning, and utilized Numpy and Pandas for data analysis.'}, 'location': {'type': 'WEB', 'webLocation': {'url': 'https://www.datascienceportfol.io/suman/projects/9'}}, 'metadata': {'x-amz-bedrock-kb-source-uri': 'https://www.datascienceportfol.io/suman/projects/9', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AHwnTvZABXMwTVBe-LBuI', 'x-amz-bedrock-kb-data-source-id': 'ZQHAKAWVXV'}, 'score': 0.4975708}, {'content': {'text': 'Storage products, technologies, and certifications • Designed and tested Disaster Recovery Solutions with various Replication Technologies • Developed Bandwidth Calculation Tool for Disaster Recovery solutions • Performed Performance Assessment of Storage Environment and Storage Health Check Assessment • Handled multiple projects for System and Technology Group (STG), GTS, GBS ranging from architecture, assessments, performance engineering, offering development and technical project management IBM Storage SAN Consulting Design 2008-2011 Performance Consultant \\\\| Hitachi Data Systems \\\\| Kolkata, India • Worked with the Global Solution Strategy and Development Team as a Performance Consultant • Conducted benchmark raw/application performance testing on HDS Modular Storage • Authored 20+ HDS performance best practice and white papers (ESRP-Exchange 2007/2010) for HDS Sales • Trained Microsoft Exchange Rangers and designed 10+ Exchange Storage Solutions Hitachi Block Storage Performance Benchmarking SAN 2008-2011 Performance Consultant \\\\| Hitachi Data Systems \\\\| Kolkata, India • Worked with the Global Solution Strategy and Development Team as a Performance Consultant • Conducted benchmark raw/application performance testing on HDS Modular Storage • Authored 20+ HDS performance best practice and white papers (ESRP-Exchange 2007/2010) for HDS Sales • Trained Microsoft Exchange Rangers and designed 10+ Exchange Storage Solutions Hitachi Block Storage Performance Benchmarking SAN Education 2022-2024 MS in Artificial Intelligence and Machine Learning Woolf University 2003-2007 B.Tech in Electronics and Communication Engineering West Bengal University of Technology, Kolkata 2002-2003 AISSCE (XII) Kendriya Vidyalaya 2000-2001 SSE Kendriya Vidyalaya'}, 'location': {'type': 'WEB', 'webLocation': {'url': 'https://www.datascienceportfol.io/suman'}}, 'metadata': {'x-amz-bedrock-kb-source-uri': 'https://www.datascienceportfol.io/suman', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AwVrTvZABm1o3SuMbKhSe', 'x-amz-bedrock-kb-data-source-id': 'ZQHAKAWVXV'}, 'score': 0.4918895}, {'content': {'text': \"Suman Debnath | Efficient Feature Engineering for a Leading Logistics Organization Suman Debnath Principal Developer Advocate (AI/ML) Based in Boston, US LinkedIn](https://www.linkedin.com/in/suman-d/)[ email me About Suman Debnath is a Principal Machine Learning Advocate at Amazon Web Services. He transitioned to machine learning in 2020 after a career in systems, storage and performance engineering. Currently, his focus is on Supervised Learning, Natural Language Processing (NLP), Large Language Models (LLMs), and Retrieval Augmented Generation (RAG). Suman is committed to leveraging open-source tools like TensorFlow, PyTorch, Numpy, Spark, and Pandas for advancing machine learning. He has developed performance benchmarking and monitoring tools for distributed storage systems. Suman has spoken at over 250 global events, including PyCon, PyData, ODSC, and meetups across multiple countries. At present, he leads the Global Developer Advocate team for Machine Learning and GenAI at AWS. Skills Python SQL AWS PyTorch TensorFlow Amazon Bedrock Amazon SageMaker Amazon EFS Linear Algebra Probability Deep Learning NLP LLM LangChain Docker Apache Spark Numpy Pandas Supervised Machine Learning Embedding Vector Database Amazon Aurora RAG GenAI back Efficient Feature Engineering for a Leading Logistics Organization Project photo Project links GitHub Skills Feature Engineering Data Cleaning Exploratory Data Analysis Python Pandas Data Manipulation About this project Cleaned and engineered features from raw data to enhance forecasting models for a leading logistics company's operations. Conducted in-depth analysis, including outlier detection, normalization, and categorical value handling, to optimize delivery route planning and improve operational efficiency.\"}, 'location': {'type': 'WEB', 'webLocation': {'url': 'https://www.datascienceportfol.io/suman/projects/5'}}, 'metadata': {'x-amz-bedrock-kb-source-uri': 'https://www.datascienceportfol.io/suman/projects/5', 'x-amz-bedrock-kb-chunk-id': '1%3A0%3AuFrTvZABm1o3SuMbGRSQ', 'x-amz-bedrock-kb-data-source-id': 'ZQHAKAWVXV'}, 'score': 0.48083284}]\n"
     ]
    }
   ],
   "source": [
    "print(relevant_documents[\"retrievalResults\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Clean up\n",
    "Please make sure to comment the below section if you are planning to use the Knowledge Base that you created above for building your RAG application.\n",
    "If you only wanted to try out creating the KB using SDK, then please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete KnowledgeBase\n",
    "# bedrock_agent_client.delete_data_source(dataSourceId = ds[\"dataSourceId\"], knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "# bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "# oss_client.indices.delete(index=index_name)\n",
    "# aoss_client.delete_collection(id=collection_id)\n",
    "# aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "# aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "# aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete role and policies\n",
    "# from utility import delete_iam_role_and_policies\n",
    "# delete_iam_role_and_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
